{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'AA']]\n",
      "===========start============\n",
      "AA.csv simulation AA\n",
      "get_csv_data: done\n",
      "==========predictresult pass========\n",
      "=====simulation pass=====\n",
      "92 92\n",
      "=======output pass=======\n",
      "[['Date', 'Prediction', 'True_price'], [u'2015-11-19', 8.722296, 8.722296], [u'2015-11-20', 8.917456362755491, 8.652596], [u'2015-11-23', 9.11962073642547, 9.030961], [u'2015-11-24', 9.35379450286787, 9.050875], [u'2015-11-25', 9.590326143868722, 9.369498], [u'2015-11-27', 9.85309749543809, 9.279885], [u'2015-11-30', 10.12728409459718, 9.319713], [u'2015-12-01', 10.250367496307865, 9.439196], [u'2015-12-02', 10.547840367422165, 9.309756], [u'2015-12-03', 10.807498135923503, 8.951305], [u'2015-12-04', 11.120064182933135, 9.299799], [u'2015-12-07', 11.408000876000822, 9.00109], [u'2015-12-08', 11.671375652332191, 8.483329], [u'2015-12-09', 11.997060708482131, 8.702381], [u'2015-12-10', 12.337257306217571, 9.130531], [u'2015-12-11', 12.616782944511531, 8.821865], [u'2015-12-14', 12.819194865447217, 8.961262], [u'2015-12-15', 12.959945454513784, 9.080746], [u'2015-12-16', 13.462244036918229, 9.200229], [u'2015-12-17', 13.620857888031214, 9.110616], [u'2015-12-18', 14.19513300135648, 9.190272], [u'2015-12-21', 14.34100922133684, 9.279885], [u'2015-12-22', 14.615315267156682, 9.518853], [u'2015-12-23', 15.046796149448546, 10.14614], [u'2015-12-24', 15.485246988462233, 10.076442], [u'2015-12-28', 15.947380848107121, 9.937044], [u'2015-12-29', 16.256654567957465, 10.086399], [u'2015-12-30', 16.556941012669057, 9.91713], [u'2015-12-31', 16.92752308219183, 9.827518], [u'2016-01-04', 17.389203367594675, 9.668207], [u'2016-01-05', 17.869235341403524, 9.230101], [u'2016-01-06', 18.207669375585773, 8.572941], [u'2016-01-07', 18.69929727473499, 8.234405], [u'2016-01-08', 18.96741560200899, 8.035265], [u'2016-01-11', 19.724618436379014, 7.965567], [u'2016-01-12', 20.39358107814674, 7.248666], [u'2016-01-13', 20.84158876337444, 7.099311], [u'2016-01-14', 21.144701715651124, 7.208838], [u'2016-01-15', 21.903537330353576, 6.870301], [u'2016-01-19', 22.676242547191688, 6.71099], [u'2016-01-20', 22.76211503326754, 6.71099], [u'2016-01-21', 23.418902857808128, 7.059484], [u'2016-01-22', 23.587463596640344, 6.84043], [u'2016-01-25', 23.746345496627892, 6.770732], [u'2016-01-26', 25.25494864183305, 7.109268], [u'2016-01-27', 25.483309398171176, 6.930043], [u'2016-01-28', 25.877918655599814, 6.969871], [u'2016-01-29', 26.68799536497381, 7.258623], [u'2016-02-01', 27.865872997085294, 7.178967], [u'2016-02-02', 27.628517325969238, 6.94], [u'2016-02-03', 28.342160048804033, 7.55], [u'2016-02-04', 29.604469092483033, 8.31], [u'2016-02-05', 30.27072507267824, 8.12], [u'2016-02-08', 30.481118619125724, 7.9], [u'2016-02-09', 31.566475471500528, 7.81], [u'2016-02-10', 32.25632513980438, 7.54], [u'2016-02-11', 34.18096111675901, 7.33], [u'2016-02-12', 33.783120042206505, 7.69], [u'2016-02-16', 35.79606239551572, 8.12], [u'2016-02-17', 34.48772854966396, 8.52], [u'2016-02-18', 36.21307112674615, 8.12], [u'2016-02-19', 36.93829063213051, 7.87], [u'2016-02-22', 38.79869314502931, 8.91], [u'2016-02-23', 40.31965254729557, 8.53], [u'2016-02-24', 40.72747930876003, 8.76], [u'2016-02-25', 41.33691085957295, 8.87], [u'2016-02-26', 42.38950806461154, 8.87], [u'2016-02-29', 43.652563839939326, 8.93], [u'2016-03-01', 43.59571905855068, 9.11], [u'2016-03-02', 45.26856232140722, 9.62], [u'2016-03-03', 47.265464155863334, 9.47], [u'2016-03-04', 48.51716262421015, 9.57], [u'2016-03-07', 49.08843031101218, 10.04], [u'2016-03-08', 50.021439400894614, 9.33], [u'2016-03-09', 50.46145059315931, 9.42], [u'2016-03-10', 52.66745305370635, 9.59], [u'2016-03-11', 53.778363084822466, 9.52], [u'2016-03-14', 55.10026582131921, 9.67], [u'2016-03-15', 55.88083270801229, 9.16], [u'2016-03-16', 57.67616413513743, 9.74], [u'2016-03-17', 58.795730277712615, 9.94], [u'2016-03-18', 60.55892074333004, 10.03], [u'2016-03-21', 60.16995918714672, 9.89], [u'2016-03-22', 63.17283855125638, 9.85], [u'2016-03-23', 65.2570240277495, 9.32], [u'2016-03-24', 66.48893416633652, 9.57], [u'2016-03-28', 68.13465880663858, 9.7], [u'2016-03-29', 71.56437891546386, 9.72], [u'2016-03-30', 73.15383479879165, 9.68], [u'2016-03-31', 73.12810017822669, 9.58], [u'2016-04-01', 77.14034675871493, 9.63]]\n",
      "AA.csv\n",
      "No service for this stock\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from elasticsearch import Elasticsearch\n",
    "import sys\n",
    "from operator import add\n",
    "from pyspark import SparkContext\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "\n",
    "#period: the days of period you choose\n",
    "#✅WELL DONE\n",
    "def get_csv_data(filename):\n",
    "    filename = \"file:/Users/nancywu/sparkhadoop/datatest/\"+filename+\".csv\"\n",
    "    File = sc.textFile(filename)\n",
    "    File.map(lambda line: line.split(\",\"))\n",
    "    File.filter(lambda line: len(line) > 0)\n",
    "    File.map(lambda line: (line[0], line[1]))\n",
    "    #print(File.first())\n",
    "    data = File.collect()\n",
    "    stock_text = [d.split(\",\") for d in data]\n",
    "    #print(stock_text[1][1])\n",
    "    start, period = 1, 90\n",
    "    predict_period = 2\n",
    "    open_price = [float(stock_text[i][1]) for i in range(start, start + period)]\n",
    "    close_price = [float(stock_text[i][4]) for i in range(start, start + period)]\n",
    "    S0 = float(stock_text[91][-1])\n",
    "    #print (open_price)\n",
    "    True_price = [\"True_price\"]\n",
    "    [True_price.append(float(stock_text[j][-1])) for j in range(1, period + predict_period)[::-1]]\n",
    "    #print(True_price)\n",
    "    print (\"get_csv_data: done\")\n",
    "    #print ( open_price,close_price,S0,True_price)\n",
    "    return open_price,close_price,S0,True_price\n",
    "\n",
    "#mu: the mean of sample training size\n",
    "#v:\n",
    "#NUMPY WRONG!!solved it already on Nov.28\n",
    "\n",
    "def simulation(filename):\n",
    "    open_price,close_price,S0,True_price = get_csv_data(filename)\n",
    "    #print (\"mu\")\n",
    "    mu= (np.array(open_price).mean()+np.array(close_price).mean())*0.5\n",
    "    #print(mu)\n",
    "    t = 1.0/365\n",
    "    #v = ((np.array(open_price).var() + np.array(close_price).var())/(np.array(open_price).mean()+np.array(close_price).mean())) * 0.5\n",
    "    #print (\"various\")\n",
    "    open_close_average_list = list(map(lambda x: float(x[0]+x[1])*0.5, zip(open_price, close_price)))\n",
    "    v= np.array(open_close_average_list).var()\n",
    "    #print (v)\n",
    "    n,M = 90,1000\n",
    "    result = [\"Prediction\"]\n",
    "    for i in range(n+1):\n",
    "        prediction = sc.parallelize(np.random.normal(0,1,M)).map(lambda x: (mu-0.5*v)*t*i+v*x*(t*i)**0.5).map(lambda x: math.exp(x)).map(lambda x: S0*x)\n",
    "        result.append(prediction.mean())\n",
    "    print (\"==========predictresult pass========\")\n",
    "    return True_price, result\n",
    "\n",
    "def generation_output(True_price,prediction):\n",
    "    Date = [\"Date\", u'2015-11-19', u'2015-11-20', u'2015-11-23', u'2015-11-24', u'2015-11-25', u'2015-11-27',\n",
    "                       u'2015-11-30', u'2015-12-01', u'2015-12-02', u'2015-12-03', u'2015-12-04', u'2015-12-07',\n",
    "                       u'2015-12-08', u'2015-12-09', u'2015-12-10', u'2015-12-11', u'2015-12-14', u'2015-12-15',\n",
    "                       u'2015-12-16', u'2015-12-17', u'2015-12-18', u'2015-12-21', u'2015-12-22', u'2015-12-23',\n",
    "                       u'2015-12-24', u'2015-12-28', u'2015-12-29', u'2015-12-30', u'2015-12-31', u'2016-01-04',\n",
    "                       u'2016-01-05', u'2016-01-06', u'2016-01-07', u'2016-01-08', u'2016-01-11', u'2016-01-12',\n",
    "                       u'2016-01-13', u'2016-01-14', u'2016-01-15', u'2016-01-19', u'2016-01-20', u'2016-01-21',\n",
    "                       u'2016-01-22', u'2016-01-25', u'2016-01-26', u'2016-01-27', u'2016-01-28', u'2016-01-29',\n",
    "                       u'2016-02-01', u'2016-02-02', u'2016-02-03', u'2016-02-04', u'2016-02-05', u'2016-02-08',\n",
    "                       u'2016-02-09', u'2016-02-10', u'2016-02-11', u'2016-02-12', u'2016-02-16', u'2016-02-17',\n",
    "                       u'2016-02-18', u'2016-02-19', u'2016-02-22', u'2016-02-23', u'2016-02-24', u'2016-02-25',\n",
    "                       u'2016-02-26', u'2016-02-29', u'2016-03-01', u'2016-03-02', u'2016-03-03', u'2016-03-04',\n",
    "                       u'2016-03-07', u'2016-03-08', u'2016-03-09', u'2016-03-10', u'2016-03-11', u'2016-03-14',\n",
    "                       u'2016-03-15', u'2016-03-16', u'2016-03-17', u'2016-03-18', u'2016-03-21', u'2016-03-22',\n",
    "                       u'2016-03-23', u'2016-03-24', u'2016-03-28', u'2016-03-29', u'2016-03-30', u'2016-03-31',\n",
    "                       u'2016-04-01']\n",
    "    output = []\n",
    "    for i in range(len(Date)):\n",
    "        tmp = [Date[i],prediction[i],True_price[i]]\n",
    "        output.append(tmp)\n",
    "    return output\n",
    "#Do not  test it yet⚠️\n",
    "#def writeToElastic(fileindex,es,filename,stock_text):\n",
    "    index_list = stock_text[0]\n",
    "    for i in stock_text[1:]:\n",
    "        d = {}\n",
    "        for x, y in zip(index_list, i):\n",
    "            if x == \"Date\":\n",
    "                Date = y\n",
    "                d[x] = y\n",
    "            else:\n",
    "                d[x] = float(y)\n",
    "        es.index(index=fileindex, doc_type=filename, id=Date, body=d)\n",
    "        \n",
    "if __name__ ==\"__main__\":\n",
    "    #sc = SparkContext(appName=\"Monte Carlo\")\n",
    "    Ticker = sc.textFile(\"file:/Users/nancywu/sparkhadoop/datatest/Tickertest.csv\")\n",
    "    filelist = Ticker.map(lambda f: f.split(\",\")).collect()\n",
    "    #l = Ticker.collect()\n",
    "    #filelist = l[0].split(\",\")\n",
    "    print(filelist)\n",
    "    es = Elasticsearch()\n",
    "    print(\"===========start============\")\n",
    "    for f in filelist:\n",
    "        try:\n",
    "            name = f[0]+\".csv\"\n",
    "            print (name,\"simulation\",f[0])\n",
    "            True_price,prediction = simulation(f[0])\n",
    "            print(\"=====simulation pass=====\")\n",
    "            print(len(True_price),len(prediction))\n",
    "            output = generation_output(True_price,prediction)\n",
    "            print(\"=======output pass=======\")\n",
    "            print (output)\n",
    "            print (name)\n",
    "            sc.parallelize(output).repartition(1).saveAsTextFile(\"file:/Users/nancywu/sparkhadoop/datatest_result/\" + name)\n",
    "            #writeToElastic(\"predictvalue\",es,name,output)\n",
    "        except:\n",
    "            print(\"No service for this stock\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
