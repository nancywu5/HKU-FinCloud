{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'AAPL']]\n",
      "===========start============\n",
      "AAPL.csv simulation AAPL\n",
      "get_csv_data: done\n",
      "====risk assessment initial======\n",
      "==========Change list=======================\n",
      "[(-5.5, 4), (7.5, 4), (6.8, 3), (0.5, 22), (-10.6, 1), (8.0, 2), (17.7, 1), (9.5, 1), (7.8, 3), (2.4, 4), (-3.4, 6), (4.5, 3), (-8.6, 1), (-3.9, 4), (-2.9, 7), (4.0, 1), (-9.6, 2), (0.0, 21), (-12.0, 1), (3.5, 2), (-0.5, 10), (11.5, 1), (2.9, 6), (-13.1, 1), (17.5, 1), (6.5, 4), (-9.1, 1), (8.5, 1), (5.5, 3), (-0.3, 19), (6.3, 2), (4.3, 9), (-4.8, 4), (7.3, 2), (-6.5, 2), (-8.1, 3), (-7.3, 3), (-11.5, 1), (-2.4, 9), (-9.5, 1), (-10.5, 2), (-1.5, 18), (1.5, 14), (-7.8, 1), (4.8, 5), (12.6, 2), (-5.8, 3), (-6.8, 1), (0.3, 18), (0.6, 8), (11.6, 4), (-0.6, 22), (8.6, 2), (-10.1, 2), (-8.5, 1), (-5.3, 5), (1.7, 10), (-15.1, 1), (-4.3, 4), (-2.5, 7), (2.5, 9), (10.5, 2), (-13.6, 1), (-4.0, 2), (-3.5, 3), (9.1, 1), (1.2, 9), (-1.7, 5), (21.2, 1), (10.6, 1), (11.1, 1), (-1.2, 14), (-8.0, 3), (5.3, 3), (3.9, 5), (-7.5, 2), (5.8, 6), (-4.5, 4), (1.0, 14), (5.0, 8), (9.0, 1), (9.2, 1), (13.0, 1), (-26.8, 1), (1.9, 4), (-2.6, 6), (22.9, 1), (4.1, 2), (18.9, 1), (6.6, 1), (-6.2, 4), (-7.2, 1), (5.6, 3), (-9.9, 1), (-5.2, 2), (15.2, 1), (3.3, 2), (-11.9, 1), (-4.2, 1), (-18.3, 1), (2.3, 9), (-3.6, 3), (8.2, 2), (-8.9, 2), (-6.7, 2), (-7.7, 2), (18.4, 2), (-14.4, 2), (-16.8, 1), (-4.7, 3), (4.6, 2), (6.1, 1), (5.1, 1), (-5.7, 5), (-11.4, 1), (29.9, 1), (-1.3, 8), (3.8, 3), (-30.8, 1), (-9.4, 2), (-3.1, 5), (7.1, 3), (8.7, 4), (1.4, 8), (7.6, 1), (-12.4, 1), (0.7, 15), (14.7, 1), (-21.3, 1), (-11.0, 1), (-0.9, 21), (-7.0, 1), (2.8, 11), (-1.8, 13), (-2.1, 11), (-3.0, 5), (2.0, 7), (-1.6, 11), (6.0, 3), (10.8, 1), (0.1, 18), (10.0, 3), (11.8, 3), (-9.3, 1), (14.0, 1), (12.8, 1), (-2.7, 6), (13.8, 1), (3.2, 5), (2.7, 2), (-10.8, 1), (14.8, 1), (-4.9, 4), (-0.2, 22), (-14.8, 1), (5.4, 5), (0.4, 13), (-4.4, 1), (-6.9, 3), (3.7, 4), (-9.8, 2), (-13.8, 1), (19.6, 1), (-6.4, 1), (-18.6, 1), (15.8, 1), (-3.2, 3), (4.4, 5), (-3.7, 5), (-0.8, 12), (0.8, 20), (8.3, 2), (-21.6, 2), (9.8, 1), (6.9, 1), (7.4, 4), (-2.0, 8), (4.9, 2), (-8.8, 1), (1.6, 8), (-2.2, 5), (-8.3, 2), (-0.4, 9), (5.9, 1), (6.4, 1), (8.8, 2), (10.3, 2), (-1.1, 21), (19.1, 1), (-10.3, 1), (20.1, 1), (7.9, 1), (16.6, 2), (11.3, 1), (12.3, 1), (-6.0, 5), (-14.3, 1), (1.1, 15), (-7.4, 5), (-0.1, 18), (30.1, 1), (-1.0, 12), (-10.0, 1), (-5.4, 3), (0.2, 26), (-5.9, 1), (-11.3, 1), (2.2, 8), (17.6, 1), (-8.2, 2), (3.6, 3), (3.0, 6), (7.0, 3), (11.0, 1), (-4.1, 1), (6.7, 1), (7.2, 4), (12.4, 1), (-6.6, 5), (8.9, 1), (-1.4, 8), (2.1, 5), (5.2, 2), (12.9, 1), (-8.7, 1), (1.8, 7), (-2.3, 5), (7.7, 1), (9.4, 1), (-2.8, 5), (-9.7, 2), (0.9, 16), (4.2, 5), (8.4, 2), (-1.9, 6), (-5.6, 2), (-5.1, 2), (1.3, 9), (11.9, 2), (-7.6, 1), (-9.2, 1), (4.7, 2), (-0.7, 21), (-3.8, 2), (-17.0, 1), (-6.1, 1), (2.6, 10), (-25.0, 1), (3.1, 2), (-5.0, 2), (10.9, 1), (-10.7, 1)]\n",
      "=====risk assessment pass=====\n",
      "252\n",
      "=======output pass=======\n",
      "AAPL.csv\n",
      "No service for this stock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-23-6963714e37cf>\", line 132, in <module>\n",
      "    sc.parallelize(change_list).repartition(1).saveAsTextFile(\"file:/Users/nancywu/sparkhadoop/datatest_result/\" + name)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/pyspark/rdd.py\", line 1506, in saveAsTextFile\n",
      "    keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\", line 813, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/pyspark/sql/utils.py\", line 45, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/lib/py4j-0.9-src.zip/py4j/protocol.py\", line 308, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "Py4JJavaError: An error occurred while calling o1348.saveAsTextFile.\n",
      ": org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/Users/nancywu/sparkhadoop/datatest_result/AAPL.csv already exists\n",
      "\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1179)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1156)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1443)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1422)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1422)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1422)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:507)\n",
      "\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n",
      "\tat sun.reflect.GeneratedMethodAccessor63.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:259)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#\n",
    "#Author: WU Nan\n",
    "#Date: 2016.12.5\n",
    "#Description: This is the csv to csv programme file to \n",
    "#Further target: \n",
    "#1) should calulate \"whether it is Normal Distribution or not \"  \n",
    "#2) Give data visualization\n",
    "#Give the very normal risk ranking\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "from __future__ import print_function\n",
    "from elasticsearch import Elasticsearch\n",
    "import sys\n",
    "from operator import add\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD, LabeledPoint\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "\n",
    "#period: the days of period you choose\n",
    "#✅WELL DONE\n",
    "def get_csv_data(filename):\n",
    "    filename = \"file:/Users/nancywu/sparkhadoop/datatest/\"+filename+\".csv\"\n",
    "    File = sc.textFile(filename)\n",
    "    File.map(lambda line: line.split(\",\"))\n",
    "    File.filter(lambda line: len(line) > 0)\n",
    "    File.map(lambda line: (line[0], line[1]))\n",
    "    data = File.collect()\n",
    "    stock_text = [d.split(\",\") for d in data]\n",
    "\n",
    "    #From backward of Period of present date (90 days before 2016.4.1) to present date 2016.4.1\n",
    "    #historical period you want to choose\n",
    "    period = 365*3\n",
    "\n",
    "    open_price = [round(float(stock_text[i][1]),32) for i in range(period,0,-1)]\n",
    "    close_price = [round(float(stock_text[i][4]),32) for i in range(period,0,-1)]\n",
    "    volume=[long(stock_text[i][5]) for i in range (period, 0,-1)]\n",
    "    date=[\"Date\"]\n",
    "    [date.append(stock_text[i][0]) for i in range(period,0,-1)]\n",
    "\n",
    "    print (\"get_csv_data: done\")\n",
    "    #print ( open_price,close_price,S0,True_price)\n",
    "    return open_price,close_price,volume,date\n",
    "\n",
    "#mu: the mean of sample training size\n",
    "#v:\n",
    "#NUMPY WRONG!!solved it already on Nov.28   ✅\n",
    "\n",
    "def risk_assessment_price_change (filename):\n",
    "    open_price,close_price, volume, date= get_csv_data(filename)\n",
    "    \n",
    "    risk_assessment_initial = list(map(lambda x: round((x[0]-x[1]),1), zip(open_price, close_price)))\n",
    "    print(\"====risk assessment initial======\")\n",
    "    tmp=sc.parallelize(risk_assessment_initial)\n",
    "    Change_initial = tmp.map(lambda distance: (distance, 1))\\\n",
    "            .reduceByKey(lambda a, b: a + b)\n",
    "    Change_list=Change_initial.collect()\n",
    "    \n",
    "    print (\"==========Change list=======================\")\n",
    "    print(Change_list)\n",
    "    return Change_list\n",
    "\n",
    "def generation_output(True_price,prediction, Date):\n",
    "    #Date=\n",
    "    output = []\n",
    "    for i in range(1,len(Date)-2):\n",
    "        #features=训练集,这里可以自己调整去做尝试； label=target目标值\n",
    "        tmp = LabeledPoint(label=True_price[i],features=[prediction[i],prediction[i+1],prediction[i+2]])\n",
    "        output.append(tmp)\n",
    "    \n",
    "    return output_model\n",
    "\n",
    "#Do not  test it yet⚠️\n",
    "def writeToElastic(fileindex,es,filename,stock_text):\n",
    "    df=stock_text\n",
    "    j = 1\n",
    "    actions = []\n",
    "    count = int(len(df))\n",
    "    while (j < count):\n",
    "        action = {\n",
    "                   \"_index\": fileindex, # 这里不可以是大写，都是小写\n",
    "                   \"_type\": filename,\n",
    "                   \"_id\": j,\n",
    "                   \"_source\": {\n",
    "                               \"date\":df[j][0],\n",
    "                               \"open\":float(df[j][1]),\n",
    "                               \"high\":float(df[j][2]),\n",
    "                               \"low\":float(df[j][3]),\n",
    "                               \"close\":float(df[j][4]),\n",
    "                               \"volume\":int(df[j][5]),\n",
    "                               \"adjClose\":float(df[j][6]),\n",
    "                               #\"timestamp\": datetime.now()\n",
    "                                }\n",
    "                   }\n",
    "        print(action)\n",
    "        actions.append(action)\n",
    "        j += 1\n",
    "        if (len(actions) == 180):\n",
    "            helpers.bulk(es, actions)\n",
    "            del actions[0:len(actions)]\n",
    "            \n",
    "    if (len(actions) >0 ):\n",
    "            helpers.bulk(es, actions)\n",
    "            del actions[0:len(actions)]\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "if __name__ ==\"__main__\":\n",
    "    #sc = SparkContext(appName=\"Monte Carlo\")\n",
    "    Ticker = sc.textFile(\"file:/Users/nancywu/sparkhadoop/datatest/Tickertest.csv\")\n",
    "    filelist = Ticker.map(lambda f: f.split(\",\")).collect()\n",
    "    #l = Ticker.collect()\n",
    "    #filelist = l[0].split(\",\")\n",
    "    print(filelist)\n",
    "    es = Elasticsearch()\n",
    "    print(\"===========start============\")\n",
    "    for f in filelist:\n",
    "        try:\n",
    "            name = f[0]+\".csv\"\n",
    "            print (name,\"simulation\",f[0])\n",
    "            change_list = risk_assessment_price_change(f[0])\n",
    "            print(\"=====risk assessment pass=====\")\n",
    "            print(len(change_list))\n",
    "            #output = generation_output(True_price,prediction, Date)\n",
    "            print(\"=======output pass=======\")\n",
    "            print (name)\n",
    "            sc.parallelize(change_list).repartition(1).saveAsTextFile(\"file:/Users/nancywu/sparkhadoop/datatest_result/\" + name)\n",
    "            #writeToElastic(\"predictvalue\",es,name,output)\n",
    "        except:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print(\"No service for this stock\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
