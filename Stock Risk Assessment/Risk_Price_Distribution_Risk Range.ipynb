{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'AA'], [u'AAPL'], [u'ZWELCAST.BO'], [u'ZYA.DU'], [u'ZYA.MU'], [u'ZYA.SG'], [u'ZYB.BE'], [u'ZZZ.L'], [u'600000.SS']]\n",
      "===========start============\n",
      "AA.csv simulation AA\n",
      "get_csv_data: done\n",
      "====risk assessment initial======\n",
      "==========Change list=======================\n",
      "=====risk assessment pass=====\n",
      "112\n",
      "[['', 'DF', 'Test Statistic_SW', 'p-value'], ['Sample Data', 111, 0.8173558712005615, 0.0]]\n",
      "[['', 'DF', 'Test Statistic_KS', 'p-value'], ['Sample Data', 111, 0.84134474606854293, 0.0]]\n",
      "[['', 'DF', 'Test Statistic_dp', 'p-value'], ['Sample Data', 111, 17.103638345751705, 0.00019319]]\n",
      "=======output pass=======\n",
      "==========Low Risk Stock===========:== AA\n",
      "No service for this stock\n",
      "AAPL.csv simulation AAPL\n",
      "get_csv_data: done\n",
      "====risk assessment initial======\n",
      "==========Change list======================="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-22-1a737ff00889>\", line 206, in <module>\n",
      "    sc.parallelize(change_list).repartition(1).saveAsTextFile(\"file:/Users/nancywu/sparkhadoop/datatest_result/\" + 'risk_change_list'+name)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/pyspark/rdd.py\", line 1506, in saveAsTextFile\n",
      "    keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\", line 813, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/pyspark/sql/utils.py\", line 45, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/lib/py4j-0.9-src.zip/py4j/protocol.py\", line 308, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "Py4JJavaError: An error occurred while calling o5757.saveAsTextFile.\n",
      ": org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/Users/nancywu/sparkhadoop/datatest_result/risk_change_listAA.csv already exists\n",
      "\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1179)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1156)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1443)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1422)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1422)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1422)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:507)\n",
      "\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n",
      "\tat sun.reflect.GeneratedMethodAccessor68.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:259)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====risk assessment pass=====\n",
      "762\n",
      "[['', 'DF', 'Test Statistic_SW', 'p-value'], ['Sample Data', 761, 0.6097652316093445, 0.0]]\n",
      "[['', 'DF', 'Test Statistic_KS', 'p-value'], ['Sample Data', 761, 0.84134474606854293, 0.0]]\n",
      "[['', 'DF', 'Test Statistic_dp', 'p-value'], ['Sample Data', 761, 316.67864975646677, 0.0]]\n",
      "=======output pass=======\n",
      "==========Netural Risk Stock===========:== AAPL\n",
      "No service for this stock"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<ipython-input-22-1a737ff00889>\", line 206, in <module>\n",
      "    sc.parallelize(change_list).repartition(1).saveAsTextFile(\"file:/Users/nancywu/sparkhadoop/datatest_result/\" + 'risk_change_list'+name)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/pyspark/rdd.py\", line 1506, in saveAsTextFile\n",
      "    keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\", line 813, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/pyspark/sql/utils.py\", line 45, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/lib/py4j-0.9-src.zip/py4j/protocol.py\", line 308, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "Py4JJavaError: An error occurred while calling o5836.saveAsTextFile.\n",
      ": org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/Users/nancywu/sparkhadoop/datatest_result/risk_change_listAAPL.csv already exists\n",
      "\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1179)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1156)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1443)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1422)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1422)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1422)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:507)\n",
      "\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n",
      "\tat sun.reflect.GeneratedMethodAccessor68.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:259)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ZWELCAST.BO.csv simulation ZWELCAST.BO\n",
      "No service for this stock\n",
      "ZYA.DU.csv simulation ZYA.DU\n",
      "No service for this stock\n",
      "ZYA.MU.csv simulation ZYA.MU\n",
      "get_csv_data: done\n",
      "====risk assessment initial======\n",
      "==========Change list======================="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<ipython-input-22-1a737ff00889>\", line 190, in <module>\n",
      "    change_list,distance_list,frequency_list = risk_assessment_price_change(f[0])\n",
      "  File \"<ipython-input-22-1a737ff00889>\", line 58, in risk_assessment_price_change\n",
      "    open_price,close_price, volume, date= get_csv_data(filename)\n",
      "  File \"<ipython-input-22-1a737ff00889>\", line 38, in get_csv_data\n",
      "    data = File.collect()\n",
      "  File \"/Users/nancywu/sparkhadoop/python/pyspark/rdd.py\", line 771, in collect\n",
      "    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \"/Users/nancywu/sparkhadoop/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\", line 813, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/pyspark/sql/utils.py\", line 45, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/lib/py4j-0.9-src.zip/py4j/protocol.py\", line 308, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/Users/nancywu/sparkhadoop/datatest/largetest/ZWELCAST.BO.csv\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\n",
      "\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n",
      "\tat scala.Option.getOrElse(Option.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n",
      "\tat scala.Option.getOrElse(Option.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.GeneratedMethodAccessor49.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:259)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-22-1a737ff00889>\", line 190, in <module>\n",
      "    change_list,distance_list,frequency_list = risk_assessment_price_change(f[0])\n",
      "  File \"<ipython-input-22-1a737ff00889>\", line 58, in risk_assessment_price_change\n",
      "    open_price,close_price, volume, date= get_csv_data(filename)\n",
      "  File \"<ipython-input-22-1a737ff00889>\", line 45, in get_csv_data\n",
      "    open_price = [round(float(stock_text[i][1]),10) for i in range(period,0,-1)]\n",
      "IndexError: list index out of range\n",
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====risk assessment pass=====\n",
      "166\n",
      "[['', 'DF', 'Test Statistic_SW', 'p-value'], ['Sample Data', 164, 0.7608832120895386, 0.0]]\n",
      "[['', 'DF', 'Test Statistic_KS', 'p-value'], ['Sample Data', 164, 0.84134474606854293, 0.0]]\n",
      "[['', 'DF', 'Test Statistic_dp', 'p-value'], ['Sample Data', 164, 53.633374728858819, 0.0]]\n",
      "=======output pass=======\n",
      "==========Netural Risk Stock===========:== ZYA.MU\n",
      "No service for this stock\n",
      "ZYA.SG.csv simulation ZYA.SG\n",
      "get_csv_data: done\n",
      "====risk assessment initial======\n",
      "==========Change list======================="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<ipython-input-22-1a737ff00889>\", line 206, in <module>\n",
      "    sc.parallelize(change_list).repartition(1).saveAsTextFile(\"file:/Users/nancywu/sparkhadoop/datatest_result/\" + 'risk_change_list'+name)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/pyspark/rdd.py\", line 1506, in saveAsTextFile\n",
      "    keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\", line 813, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/pyspark/sql/utils.py\", line 45, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/lib/py4j-0.9-src.zip/py4j/protocol.py\", line 308, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "Py4JJavaError: An error occurred while calling o5954.saveAsTextFile.\n",
      ": org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/Users/nancywu/sparkhadoop/datatest_result/risk_change_listZYA.MU.csv already exists\n",
      "\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1179)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1156)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1443)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1422)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1422)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1422)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:507)\n",
      "\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n",
      "\tat sun.reflect.GeneratedMethodAccessor68.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:259)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-22-1a737ff00889>\", line 206, in <module>\n",
      "    sc.parallelize(change_list).repartition(1).saveAsTextFile(\"file:/Users/nancywu/sparkhadoop/datatest_result/\" + 'risk_change_list'+name)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/pyspark/rdd.py\", line 1506, in saveAsTextFile\n",
      "    keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\", line 813, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/pyspark/sql/utils.py\", line 45, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/lib/py4j-0.9-src.zip/py4j/protocol.py\", line 308, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "Py4JJavaError: An error occurred while calling o6033.saveAsTextFile.\n",
      ": org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/Users/nancywu/sparkhadoop/datatest_result/risk_change_listZYA.SG.csv already exists\n",
      "\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1179)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1156)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1443)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1422)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1422)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1422)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:507)\n",
      "\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n",
      "\tat sun.reflect.GeneratedMethodAccessor68.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:259)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====risk assessment pass=====\n",
      "208\n",
      "[['', 'DF', 'Test Statistic_SW', 'p-value'], ['Sample Data', 206, 0.7942911386489868, 0.0]]\n",
      "[['', 'DF', 'Test Statistic_KS', 'p-value'], ['Sample Data', 206, 0.84134474606854293, 0.0]]\n",
      "[['', 'DF', 'Test Statistic_dp', 'p-value'], ['Sample Data', 206, 58.71491355002113, 0.0]]\n",
      "=======output pass=======\n",
      "==========Netural Risk Stock===========:== ZYA.SG\n",
      "No service for this stock\n",
      "ZYB.BE.csv simulation ZYB.BE\n",
      "No service for this stock\n",
      "ZZZ.L.csv simulation ZZZ.L\n",
      "No service for this stock\n",
      "600000.SS.csv simulation 600000.SS\n",
      "get_csv_data: done\n",
      "====risk assessment initial======\n",
      "==========Change list======================="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<ipython-input-22-1a737ff00889>\", line 190, in <module>\n",
      "    change_list,distance_list,frequency_list = risk_assessment_price_change(f[0])\n",
      "  File \"<ipython-input-22-1a737ff00889>\", line 58, in risk_assessment_price_change\n",
      "    open_price,close_price, volume, date= get_csv_data(filename)\n",
      "  File \"<ipython-input-22-1a737ff00889>\", line 45, in get_csv_data\n",
      "    open_price = [round(float(stock_text[i][1]),10) for i in range(period,0,-1)]\n",
      "IndexError: list index out of range\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-22-1a737ff00889>\", line 190, in <module>\n",
      "    change_list,distance_list,frequency_list = risk_assessment_price_change(f[0])\n",
      "  File \"<ipython-input-22-1a737ff00889>\", line 58, in risk_assessment_price_change\n",
      "    open_price,close_price, volume, date= get_csv_data(filename)\n",
      "  File \"<ipython-input-22-1a737ff00889>\", line 38, in get_csv_data\n",
      "    data = File.collect()\n",
      "  File \"/Users/nancywu/sparkhadoop/python/pyspark/rdd.py\", line 771, in collect\n",
      "    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
      "  File \"/Users/nancywu/sparkhadoop/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\", line 813, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/pyspark/sql/utils.py\", line 45, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/lib/py4j-0.9-src.zip/py4j/protocol.py\", line 308, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/Users/nancywu/sparkhadoop/datatest/largetest/ZZZ.L.csv\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\n",
      "\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n",
      "\tat scala.Option.getOrElse(Option.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n",
      "\tat scala.Option.getOrElse(Option.scala:120)\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.GeneratedMethodAccessor49.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:259)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====risk assessment pass=====\n",
      "148\n",
      "[['', 'DF', 'Test Statistic_SW', 'p-value'], ['Sample Data', 147, 0.4235299825668335, 0.0]]\n",
      "[['', 'DF', 'Test Statistic_KS', 'p-value'], ['Sample Data', 147, 0.84134474606854293, 0.0]]\n",
      "[['', 'DF', 'Test Statistic_dp', 'p-value'], ['Sample Data', 147, 237.40201549527393, 0.0]]\n",
      "=======output pass=======\n",
      "==========Netural Risk Stock===========:== 600000.SS\n",
      "No service for this stock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<ipython-input-22-1a737ff00889>\", line 206, in <module>\n",
      "    sc.parallelize(change_list).repartition(1).saveAsTextFile(\"file:/Users/nancywu/sparkhadoop/datatest_result/\" + 'risk_change_list'+name)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/pyspark/rdd.py\", line 1506, in saveAsTextFile\n",
      "    keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\", line 813, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/pyspark/sql/utils.py\", line 45, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/Users/nancywu/sparkhadoop/python/lib/py4j-0.9-src.zip/py4j/protocol.py\", line 308, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "Py4JJavaError: An error occurred while calling o6151.saveAsTextFile.\n",
      ": org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/Users/nancywu/sparkhadoop/datatest_result/risk_change_list600000.SS.csv already exists\n",
      "\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1179)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1156)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1443)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1422)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1422)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1422)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:507)\n",
      "\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:46)\n",
      "\tat sun.reflect.GeneratedMethodAccessor68.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:259)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#\n",
    "#Author: WU Nan\n",
    "#Date: 2016.12.5\n",
    "#Description: This is the csv to csv programme file to \n",
    "#Further target: \n",
    "#1) should calulate \"whether it is Normal Distribution or not \"  \n",
    "#2) Give data visualization\n",
    "#Give the very normal risk ranking\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "from __future__ import print_function\n",
    "from elasticsearch import Elasticsearch\n",
    "import sys\n",
    "from operator import add\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD, LabeledPoint\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "\n",
    "#period: the days of period you choose\n",
    "#✅WELL DONE\n",
    "def get_csv_data(filename):\n",
    "    filename = \"file:/Users/nancywu/sparkhadoop/datatest/largetest/\"+filename+\".csv\"\n",
    "    File = sc.textFile(filename)\n",
    "    File.map(lambda line: line.split(\",\"))\n",
    "    File.filter(lambda line: len(line) > 0)\n",
    "    File.map(lambda line: (line[0], line[1]))\n",
    "    data = File.collect()\n",
    "    stock_text = [d.split(\",\") for d in data]\n",
    "\n",
    "    #From backward of Period of present date (90 days before 2016.4.1) to present date 2016.4.1\n",
    "    #historical period you want to choose\n",
    "    period = 365*3\n",
    "\n",
    "    open_price = [round(float(stock_text[i][1]),10) for i in range(period,0,-1)]\n",
    "    close_price = [round(float(stock_text[i][4]),10) for i in range(period,0,-1)]\n",
    "    volume=[long(stock_text[i][5]) for i in range (period, 0,-1)]\n",
    "    date=[\"Date\"]\n",
    "    [date.append(stock_text[i][0]) for i in range(period,0,-1)]\n",
    "\n",
    "    print (\"get_csv_data: done\")\n",
    "    return open_price,close_price,volume,date\n",
    "\n",
    "\n",
    "#NUMPY WRONG!!solved it already on Nov.28   ✅\n",
    "\n",
    "def risk_assessment_price_change (filename):\n",
    "    open_price,close_price, volume, date= get_csv_data(filename)\n",
    "    \n",
    "    ##risk distance round精度调整\n",
    "    risk_assessment_initial = list(map(lambda x: round((x[0]-x[1]),2), zip(open_price, close_price)))\n",
    "    ##\n",
    "    print(\"====risk assessment initial======\")\n",
    "    tmp=sc.parallelize(risk_assessment_initial)\n",
    "    Change_initial = tmp.map(lambda distance: (distance, 1))\\\n",
    "            .reduceByKey(lambda a, b: a + b)\n",
    "    Change_list=Change_initial.collect()\n",
    "    distance_list=[]\n",
    "    frequency_list=[]\n",
    "    for (distance,frequency) in Change_list:\n",
    "        #这里限制了波动噪声。\n",
    "        if (frequency<len(risk_assessment_initial)*0.5):\n",
    "            distance_list.append(distance)\n",
    "            frequency_list.append(frequency)\n",
    "    \n",
    "    print (\"==========Change list=======================\")\n",
    "    #print(Change_list)\n",
    "    return Change_list,distance_list,frequency_list\n",
    "\n",
    "def normal_distribution_test(frequency_list):\n",
    "    \n",
    "    #3 statistics to test how distribution it is.\n",
    "    #Make risk range for particular stock\n",
    "        x = frequency_list\n",
    "        shapiro_results = scipy.stats.shapiro(x)\n",
    "        a1=shapiro_results[0]\n",
    "        a2=round(shapiro_results[1],8)\n",
    "        matrix_sw = [\n",
    "                ['', 'DF', 'Test Statistic_SW', 'p-value'],\n",
    "                ['Sample Data', len(x) - 1, a1, a2]\n",
    "                ]\n",
    "        print(matrix_sw)\n",
    "                #shapiro_result more close to 1 more probabily of normal distribution it is\n",
    "                #####\n",
    "        ks_results = scipy.stats.kstest(x, cdf='norm')\n",
    "        b1=ks_results[0]\n",
    "        b2=round(ks_results[1],8)\n",
    "        matrix_ks = [\n",
    "                ['', 'DF', 'Test Statistic_KS', 'p-value'],\n",
    "                ['Sample Data', len(x) - 1, b1, b2]\n",
    "                ]\n",
    "        \n",
    "        print(matrix_ks)\n",
    "                ###\n",
    "        dagostino_results = scipy.stats.mstats.normaltest(x)\n",
    "        c1=dagostino_results[0]\n",
    "        c2=round(dagostino_results[1],8)\n",
    "        matrix_dp = [\n",
    "                ['', 'DF', 'Test Statistic_dp', 'p-value'],\n",
    "                ['Sample Data', len(x) - 1, c1, c2]\n",
    "                ]\n",
    "        print(matrix_dp)\n",
    "        \n",
    "        if (abs(a1-a2)>0.8 and (abs(b1-b2)>0.8 and b2<0.0005) and (abs(c1-c2)>0.8 and c2<0.0005)):\n",
    "            #normal distribution. low risk\n",
    "            signal=3\n",
    "        elif (abs(a1-a2)>1 or abs(b1-b2)>1 or abs(c1-c2)>1): \n",
    "            signal=2\n",
    "        else:\n",
    "            #not normal distribution. high risk\n",
    "            signal =1\n",
    "        return signal \n",
    "    \n",
    "# Haven't done yet\n",
    "def risk_range(risk_signal,name):\n",
    "    #Date=\n",
    "    if (risk_signal==3):\n",
    "                print (\"==========Low Risk Stock===========:==\",name)\n",
    "                assessment=[name,'Low Risk']\n",
    "    if (risk_signal==2):\n",
    "                print (\"==========Netural Risk Stock===========:==\",name)\n",
    "                assessment=[name,'Netural Risk']\n",
    "    if (risk_signal==1):\n",
    "                print (\"==========High Risk Stock===========:==\",name) \n",
    "                assessment=[name,'High Risk']\n",
    "    \n",
    "    \n",
    "    return assessment\n",
    "\n",
    "#Do not  test it yet⚠️\n",
    "def writeToElastic(fileindex,es,filename,stock_text):\n",
    "    df=stock_text\n",
    "    j = 1\n",
    "    actions = []\n",
    "    count = int(len(df))\n",
    "    while (j < count):\n",
    "        action = {\n",
    "                   \"_index\": fileindex, # 这里不可以是大写，都是小写\n",
    "                   \"_type\": filename,\n",
    "                   \"_id\": j,\n",
    "                   \"_source\": {\n",
    "                               \"date\":df[j][0],\n",
    "                               \"open\":float(df[j][1]),\n",
    "                               \"high\":float(df[j][2]),\n",
    "                               \"low\":float(df[j][3]),\n",
    "                               \"close\":float(df[j][4]),\n",
    "                               \"volume\":int(df[j][5]),\n",
    "                               \"adjClose\":float(df[j][6]),\n",
    "                               #\"timestamp\": datetime.now()\n",
    "                                }\n",
    "                   }\n",
    "        print(action)\n",
    "        actions.append(action)\n",
    "        j += 1\n",
    "        if (len(actions) == 180):\n",
    "            helpers.bulk(es, actions)\n",
    "            del actions[0:len(actions)]\n",
    "            \n",
    "    if (len(actions) >0 ):\n",
    "            helpers.bulk(es, actions)\n",
    "            del actions[0:len(actions)]\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "if __name__ ==\"__main__\":\n",
    "    #sc = SparkContext(appName=\"Monte Carlo\")\n",
    "    Ticker = sc.textFile(\"file:/Users/nancywu/sparkhadoop/datatest/Tickertest.csv\")\n",
    "    filelist = Ticker.map(lambda f: f.split(\",\")).collect()\n",
    "    #l = Ticker.collect()\n",
    "    #filelist = l[0].split(\",\")\n",
    "    print(filelist)\n",
    "    es = Elasticsearch()\n",
    "    risk_list=[]\n",
    "    print(\"===========start============\")\n",
    "    for f in filelist:\n",
    "        try:\n",
    "            name = f[0]+\".csv\"\n",
    "            print (name,\"simulation\",f[0])\n",
    "            change_list,distance_list,frequency_list = risk_assessment_price_change(f[0])\n",
    "            #alist=[[\"distance\"]+distance_list,[\"freq\"]+frequency_list]\n",
    "            alist=pd.DataFrame(change_list,columns=['Price Change','Frequence'])\n",
    "            print(\"=====risk assessment pass=====\")\n",
    "            print(len(change_list))\n",
    "            #blist=pd.read_csv('file:/Users/nancywu/sparkhadoop/datatest/largetest/AA.csv')\n",
    "            #print(ggplot(alist,aes('Price Change','Frequence'))+geom_point()+stat_smooth(colour='blue', span=0.2))\n",
    "            #pl.plot(distance_list,frequency_list ,'ro')\n",
    "            #pl.xlabel(\"Price Change\")\n",
    "            #pl.ylabel(\"Frequency\")\n",
    "            #pl.title(f[0])\n",
    "            #pl.show()\n",
    "            risk_signal=normal_distribution_test(frequency_list)\n",
    "            print(\"=======output pass=======\")\n",
    "            risk_list.append(risk_range(risk_signal, f[0]))\n",
    "            ########\n",
    "            sc.parallelize(change_list).repartition(1).saveAsTextFile(\"file:/Users/nancywu/sparkhadoop/datatest_result/\" + 'risk_change_list'+name)\n",
    "            #writeToElastic(\"predictvalue\",es,name,output)\n",
    "        except:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print(\"No service for this stock\")\n",
    "    sc.parallelize(risk_list).repartition(1).saveAsTextFile(\"file:/Users/nancywu/sparkhadoop/datatest_result/stock_risk_list\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         AA  FREQ    D   DD\n",
      "0  distance  freq  NaN  NaN\n",
      "1         1     1  1.0  1.0\n",
      "2         2     2  2.0  2.0\n"
     ]
    }
   ],
   "source": [
    "x=[1,1,1,1]\n",
    "y=[2,2,2,2]\n",
    "alist=[[\"distance\",\"freq\"],x,y]\n",
    "blist=pd.read_csv('file:/Users/nancywu/sparkhadoop/datatest/largetest/AA.csv')\n",
    "alist=pd.DataFrame(alist,columns=['AA','FREQ','D','DD'])\n",
    "print (alist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
